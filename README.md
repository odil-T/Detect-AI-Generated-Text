# Detect-AI-Generated-Text
This repository stores the files used to get the personal best public score for the `LLM - Detect AI Generated Text` Kaggle Competition.

The `train_essays_chatgpt_manual.csv` file contains data that was manually collected by me from ChatGPT-3.5.
The `train_v2_drcat_02.csv` file contains data compiled from various sources. This dataset, called DAIGT-V2, was obtained from the following Kaggle User: https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset.
The `Mistral7B_CME_v7.csv` file contains data obtained from MISTRAL-7B from the following Kaggle User: https://www.kaggle.com/datasets/carlmcbrideellis/llm-mistral-7b-instruct-texts.

The `best_submission.ipynb` notebook contains the code used to submit to the Kaggle Competition that gave the best score. The score obtained was 0.794. The three datasets were combined by only including the `text` and `generated` features. The `RDizzl3_seven` feature in the DAIGT-V2 dataset was filtered for `True` values. This only includes essays with prompts that were present in the hidden test set that was used to evaluate the score. This allows for more representative data, potentially increasing the score. The text from the essay samples was preprocessed by removing punctuation and stop words and by lemmatizing the remaining tokens to produce cleaner data. Next, word2vec text representation was used to convert the text to numbers, where each sample was represented as a vector of 300 numbers. The `VotingClassifier` class from `scikit-learn` was used to collectively include several models for prediction, allowing for more robust results. The following models were included in `VotingClassifier`: `LogisticRegression`, `RandomForestClassifier`, `KNeighborsClassifier`, `GradientBoostingClassifier`, `XGBClassifier`, `LGBMClassifier`, and `CatBoostClassifier`. The voting mode was set to `soft`.
